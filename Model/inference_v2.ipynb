{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91234cfe",
   "metadata": {
    "_cell_guid": "fd1b354a-917c-4776-9773-ee2d22996d25",
    "_uuid": "6b9078b5-d216-4709-afd2-27615a5a3a57",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-02T23:25:08.414486Z",
     "iopub.status.busy": "2024-12-02T23:25:08.414210Z",
     "iopub.status.idle": "2024-12-02T23:28:41.446277Z",
     "shell.execute_reply": "2024-12-02T23:28:41.445047Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 213.039572,
     "end_time": "2024-12-02T23:28:41.448172",
     "exception": false,
     "start_time": "2024-12-02T23:25:08.408600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.4.0\r\n",
      "Uninstalling torch-2.4.0:\r\n",
      "  Successfully uninstalled torch-2.4.0\r\n",
      "Processing /kaggle/input/logits-processor-zoo/logits_processor_zoo-0.1.0-py3-none-any.whl\r\n",
      "Installing collected packages: logits-processor-zoo\r\n",
      "Successfully installed logits-processor-zoo-0.1.0\r\n",
      "CPU times: user 2.15 s, sys: 600 ms, total: 2.75 s\n",
      "Wall time: 3min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!pip uninstall -y torch\n",
    "!pip install -q --no-index --find-links=/kaggle/input/making-wheels-of-necessary-packages-for-vllm vllm\n",
    "!pip install -q -U /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!pip install -q -U /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl\n",
    "!pip install -q --no-deps --no-index /kaggle/input/hf-libraries/sentence-transformers/sentence_transformers-3.1.0-py3-none-any.whl\n",
    "!pip install --no-deps --no-index /kaggle/input/logits-processor-zoo/logits_processor_zoo-0.1.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1979f040",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T23:28:41.458460Z",
     "iopub.status.busy": "2024-12-02T23:28:41.458142Z",
     "iopub.status.idle": "2024-12-02T23:28:49.944926Z",
     "shell.execute_reply": "2024-12-02T23:28:49.943896Z"
    },
    "papermill": {
     "duration": 8.494991,
     "end_time": "2024-12-02T23:28:49.947684",
     "exception": false,
     "start_time": "2024-12-02T23:28:41.452693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers peft accelerate \\\n",
    "    -q -U --no-index --find-links /kaggle/input/lmsys-wheel-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fda2249e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T23:28:49.959130Z",
     "iopub.status.busy": "2024-12-02T23:28:49.958867Z",
     "iopub.status.idle": "2024-12-02T23:29:30.761407Z",
     "shell.execute_reply": "2024-12-02T23:29:30.760441Z"
    },
    "papermill": {
     "duration": 40.811138,
     "end_time": "2024-12-02T23:29:30.763449",
     "exception": false,
     "start_time": "2024-12-02T23:28:49.952311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --no-index /kaggle/input/bitsandbytes0-42-0/bitsandbytes-0.42.0-py3-none-any.whl --find-links=/kaggle/input/bitsandbytes0-42-0\n",
    "!pip install --no-index  /kaggle/input/bitsandbytes0-42-0/optimum-1.21.2-py3-none-any.whl --find-links=/kaggle/input/bitsandbytes0-42-0\n",
    "!pip install --no-index  /kaggle/input/bitsandbytes0-42-0/auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --find-links=/kaggle/input/bitsandbytes0-42-0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc189558",
   "metadata": {
    "papermill": {
     "duration": 0.004125,
     "end_time": "2024-12-02T23:29:30.772167",
     "exception": false,
     "start_time": "2024-12-02T23:29:30.768042",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Retrieval using 7B LLM\n",
    "Taken from: https://www.kaggle.com/code/sayoulala/use-llm-embedding-recall-infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "726c6805",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T23:29:30.781758Z",
     "iopub.status.busy": "2024-12-02T23:29:30.781477Z",
     "iopub.status.idle": "2024-12-02T23:29:31.482358Z",
     "shell.execute_reply": "2024-12-02T23:29:31.481607Z"
    },
    "papermill": {
     "duration": 0.707806,
     "end_time": "2024-12-02T23:29:31.484243",
     "exception": false,
     "start_time": "2024-12-02T23:29:30.776437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_text</th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>ConstructName</th>\n",
       "      <th>SubjectName</th>\n",
       "      <th>QuestionText</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>incorrect_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>###question###:BIDMAS-Use the order of operati...</td>\n",
       "      <td>1869_B</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>\\( 3 \\times 2+(4-5) \\)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>###question###:BIDMAS-Use the order of operati...</td>\n",
       "      <td>1869_C</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>\\( 3 \\times(2+4-5) \\)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>###question###:BIDMAS-Use the order of operati...</td>\n",
       "      <td>1869_D</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>Does not need brackets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>###question###:Simplifying Algebraic Fractions...</td>\n",
       "      <td>1870_A</td>\n",
       "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
       "      <td>Does not simplify</td>\n",
       "      <td>\\( m+1 \\)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>###question###:Simplifying Algebraic Fractions...</td>\n",
       "      <td>1870_B</td>\n",
       "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
       "      <td>Does not simplify</td>\n",
       "      <td>\\( m+2 \\)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>###question###:Simplifying Algebraic Fractions...</td>\n",
       "      <td>1870_C</td>\n",
       "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
       "      <td>Does not simplify</td>\n",
       "      <td>\\( m-1 \\)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>###question###:Range and Interquartile Range f...</td>\n",
       "      <td>1871_A</td>\n",
       "      <td>Calculate the range from a list of data</td>\n",
       "      <td>Range and Interquartile Range from a List of Data</td>\n",
       "      <td>Tom and Katie are discussing the \\( 5 \\) plant...</td>\n",
       "      <td>Only\\nKatie</td>\n",
       "      <td>Only\\nTom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>###question###:Range and Interquartile Range f...</td>\n",
       "      <td>1871_C</td>\n",
       "      <td>Calculate the range from a list of data</td>\n",
       "      <td>Range and Interquartile Range from a List of Data</td>\n",
       "      <td>Tom and Katie are discussing the \\( 5 \\) plant...</td>\n",
       "      <td>Only\\nKatie</td>\n",
       "      <td>Both Tom and Katie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>###question###:Range and Interquartile Range f...</td>\n",
       "      <td>1871_D</td>\n",
       "      <td>Calculate the range from a list of data</td>\n",
       "      <td>Range and Interquartile Range from a List of Data</td>\n",
       "      <td>Tom and Katie are discussing the \\( 5 \\) plant...</td>\n",
       "      <td>Only\\nKatie</td>\n",
       "      <td>Neither is correct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          query_text QuestionId_Answer  \\\n",
       "0  ###question###:BIDMAS-Use the order of operati...            1869_B   \n",
       "1  ###question###:BIDMAS-Use the order of operati...            1869_C   \n",
       "2  ###question###:BIDMAS-Use the order of operati...            1869_D   \n",
       "3  ###question###:Simplifying Algebraic Fractions...            1870_A   \n",
       "4  ###question###:Simplifying Algebraic Fractions...            1870_B   \n",
       "5  ###question###:Simplifying Algebraic Fractions...            1870_C   \n",
       "6  ###question###:Range and Interquartile Range f...            1871_A   \n",
       "7  ###question###:Range and Interquartile Range f...            1871_C   \n",
       "8  ###question###:Range and Interquartile Range f...            1871_D   \n",
       "\n",
       "                                       ConstructName  \\\n",
       "0  Use the order of operations to carry out calcu...   \n",
       "1  Use the order of operations to carry out calcu...   \n",
       "2  Use the order of operations to carry out calcu...   \n",
       "3  Simplify an algebraic fraction by factorising ...   \n",
       "4  Simplify an algebraic fraction by factorising ...   \n",
       "5  Simplify an algebraic fraction by factorising ...   \n",
       "6            Calculate the range from a list of data   \n",
       "7            Calculate the range from a list of data   \n",
       "8            Calculate the range from a list of data   \n",
       "\n",
       "                                         SubjectName  \\\n",
       "0                                             BIDMAS   \n",
       "1                                             BIDMAS   \n",
       "2                                             BIDMAS   \n",
       "3                    Simplifying Algebraic Fractions   \n",
       "4                    Simplifying Algebraic Fractions   \n",
       "5                    Simplifying Algebraic Fractions   \n",
       "6  Range and Interquartile Range from a List of Data   \n",
       "7  Range and Interquartile Range from a List of Data   \n",
       "8  Range and Interquartile Range from a List of Data   \n",
       "\n",
       "                                        QuestionText         correct_answer  \\\n",
       "0  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...  \\( 3 \\times(2+4)-5 \\)   \n",
       "1  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...  \\( 3 \\times(2+4)-5 \\)   \n",
       "2  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...  \\( 3 \\times(2+4)-5 \\)   \n",
       "3  Simplify the following, if possible: \\( \\frac{...      Does not simplify   \n",
       "4  Simplify the following, if possible: \\( \\frac{...      Does not simplify   \n",
       "5  Simplify the following, if possible: \\( \\frac{...      Does not simplify   \n",
       "6  Tom and Katie are discussing the \\( 5 \\) plant...            Only\\nKatie   \n",
       "7  Tom and Katie are discussing the \\( 5 \\) plant...            Only\\nKatie   \n",
       "8  Tom and Katie are discussing the \\( 5 \\) plant...            Only\\nKatie   \n",
       "\n",
       "         incorrect_answer  \n",
       "0  \\( 3 \\times 2+(4-5) \\)  \n",
       "1   \\( 3 \\times(2+4-5) \\)  \n",
       "2  Does not need brackets  \n",
       "3               \\( m+1 \\)  \n",
       "4               \\( m+2 \\)  \n",
       "5               \\( m-1 \\)  \n",
       "6               Only\\nTom  \n",
       "7      Both Tom and Katie  \n",
       "8      Neither is correct  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "full_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\")\n",
    "\n",
    "\n",
    "rows = []\n",
    "for idx, row in full_df.iterrows():\n",
    "    for option in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "        if option == row.CorrectAnswer:\n",
    "            continue\n",
    "            \n",
    "        correct_answer = row[f\"Answer{row.CorrectAnswer}Text\"]\n",
    "        # format query\n",
    "        #query_text = \"You are a helpful math teacher capable of identify the misconceptions causing the incorrect answer\\n\"\n",
    "        #query_text += \"Following is the concrete question description, including question content, correct answer, and incorrect asnwer associated with certain misconception\\n\"\n",
    "        query_text = f\"###question###:{row['SubjectName']}-{row['ConstructName']}-{row['QuestionText']}\\n###Correct Answer###:{correct_answer}\\n###Misconception Incorrect answer###:{option}.{row[f'Answer{option}Text']}\"\n",
    "\n",
    "        rows.append({\"query_text\": query_text, \n",
    "                     \"QuestionId_Answer\": f\"{row.QuestionId}_{option}\",\n",
    "                     \"ConstructName\": row.ConstructName,\n",
    "                     \"SubjectName\": row.SubjectName,\n",
    "                     \"QuestionText\": row.QuestionText,\n",
    "                     \"correct_answer\": correct_answer,\n",
    "                     \"incorrect_answer\": row[f\"Answer{option}Text\"]\n",
    "                     })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "473e8805",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T23:29:31.494910Z",
     "iopub.status.busy": "2024-12-02T23:29:31.494636Z",
     "iopub.status.idle": "2024-12-02T23:29:35.389302Z",
     "shell.execute_reply": "2024-12-02T23:29:35.388600Z"
    },
    "papermill": {
     "duration": 3.902399,
     "end_time": "2024-12-02T23:29:35.391265",
     "exception": false,
     "start_time": "2024-12-02T23:29:31.488866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from numpy.linalg import norm\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel,BitsAndBytesConfig\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "\n",
    "def batch_to_device(batch, target_device):\n",
    "    \"\"\"\n",
    "    send a pytorch batch to a device (CPU/GPU)\n",
    "    \"\"\"\n",
    "    for key in batch:\n",
    "        if isinstance(batch[key], Tensor):\n",
    "            batch[key] = batch[key].to(target_device)\n",
    "    return batch\n",
    "\n",
    "def last_token_pool(last_hidden_states: Tensor,\n",
    "                    attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    # extract the last hidden states?\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    # combine instruct + query -> full prompt\n",
    "    return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "\n",
    "def inference(df, model, tokenizer, device):\n",
    "    batch_size = 16\n",
    "    max_length = 512\n",
    "    sentences = list(df['query_text'].values)\n",
    "\n",
    "    all_embeddings = []\n",
    "    length_sorted_idx = np.argsort([-len(sen) for sen in sentences])\n",
    "    sentences_sorted = [sentences[idx] for idx in length_sorted_idx]\n",
    "    for start_index in trange(0, len(sentences), batch_size, desc=\"Batches\", disable=False):\n",
    "        sentences_batch = sentences_sorted[start_index: start_index + batch_size]\n",
    "        features = tokenizer(sentences_batch, max_length=max_length, padding=True, truncation=True,\n",
    "                             return_tensors=\"pt\")\n",
    "        features = batch_to_device(features, device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.model(**features)\n",
    "            embeddings = last_token_pool(outputs.last_hidden_state, features['attention_mask'])\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, dim=-1)\n",
    "            embeddings = embeddings.detach().cpu().numpy().tolist()\n",
    "        all_embeddings.extend(embeddings)\n",
    "\n",
    "    all_embeddings = [np.array(all_embeddings[idx]).reshape(1, -1) for idx in np.argsort(length_sorted_idx)]\n",
    "\n",
    "    return np.concatenate(all_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5488cb12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T23:29:35.401826Z",
     "iopub.status.busy": "2024-12-02T23:29:35.401435Z",
     "iopub.status.idle": "2024-12-02T23:29:35.405282Z",
     "shell.execute_reply": "2024-12-02T23:29:35.404618Z"
    },
    "papermill": {
     "duration": 0.010741,
     "end_time": "2024-12-02T23:29:35.406873",
     "exception": false,
     "start_time": "2024-12-02T23:29:35.396132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_prefix = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics\"\n",
    "model_path = \"/kaggle/input/sfr-embedding-mistral/SFR-Embedding-2_R\"\n",
    "lora_path=\"/kaggle/input/v7-recall/epoch_19_model/adapter.bin\"\n",
    "device='cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "771a938d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T23:29:35.416733Z",
     "iopub.status.busy": "2024-12-02T23:29:35.416515Z",
     "iopub.status.idle": "2024-12-02T23:30:45.614817Z",
     "shell.execute_reply": "2024-12-02T23:30:45.613851Z"
    },
    "papermill": {
     "duration": 70.20567,
     "end_time": "2024-12-02T23:30:45.617067",
     "exception": false,
     "start_time": "2024-12-02T23:29:35.411397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2031efd18edb43c98a1e79c15714e470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(lora_path.replace(\"/adapter.bin\",\"\"))\n",
    "# quantize config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "# load mistral model as a backbone?\n",
    "backbone = AutoModel.from_pretrained(model_path, quantization_config=bnb_config,device_map=device)\n",
    "config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=128,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ],\n",
    "        bias=\"none\",\n",
    "        lora_dropout=0.05,  # Conventional\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "model = get_peft_model(backbone, config)\n",
    "d = torch.load(lora_path, map_location=model.device)\n",
    "model.load_state_dict(d, strict=False)\n",
    "model = model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03b20ea6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T23:30:45.628231Z",
     "iopub.status.busy": "2024-12-02T23:30:45.627952Z",
     "iopub.status.idle": "2024-12-02T23:38:22.320936Z",
     "shell.execute_reply": "2024-12-02T23:38:22.319871Z"
    },
    "papermill": {
     "duration": 456.700634,
     "end_time": "2024-12-02T23:38:22.322765",
     "exception": false,
     "start_time": "2024-12-02T23:30:45.622131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa7ea82dafc4fd7acb857431dae9173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9998889e0928439a8a79167614b132ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2587, 4096)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm.autonotebook import trange\n",
    "\n",
    "# embedding the answer matrix\n",
    "task_description = 'Given a math question and a misconcepte incorrect answer, please retrieve the most accurate reason for the misconception.'\n",
    "V_answer = inference(df, model, tokenizer, device)\n",
    "\n",
    "# embedding the misconception matrix\n",
    "misconception_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n",
    "misconception_df[\"query_text\"] = misconception_df[\"MisconceptionName\"]\n",
    "\n",
    "V_misconception = inference(misconception_df, model, tokenizer, device)\n",
    "V_misconception.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28a23818",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T23:38:22.334137Z",
     "iopub.status.busy": "2024-12-02T23:38:22.333818Z",
     "iopub.status.idle": "2024-12-02T23:38:22.720959Z",
     "shell.execute_reply": "2024-12-02T23:38:22.719935Z"
    },
    "papermill": {
     "duration": 0.395196,
     "end_time": "2024-12-02T23:38:22.723013",
     "exception": false,
     "start_time": "2024-12-02T23:38:22.327817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2499 2142  418  405  927 2117  143  639 1535 2143  171  265 1883 1735\n",
      " 2204 1255 2046 1835 1755  396 2336 1406 1512  367 1235]\n"
     ]
    }
   ],
   "source": [
    "# use a finetuned scoring model to refine the embedding\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class ScoringNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(ScoringNN, self).__init__()\n",
    "        self.query_input = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(hidden_dim, hidden_dim//2),\n",
    "        )\n",
    "        self.mis_input = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(hidden_dim, hidden_dim//2),\n",
    "        )\n",
    "        self.criterion = torch.nn.BCELoss()\n",
    "\n",
    "    def forward(self, Q, M):\n",
    "        \n",
    "        res_Q = self.query_input(Q)\n",
    "        res_M = self.mis_input(M)\n",
    "        return torch.matmul(Q,M.T)*0.9 + torch.matmul(res_Q,res_M.T)*0.1\n",
    "\n",
    "    def loss(self, score, y):\n",
    "        # capture the diagonal elements\n",
    "        match_score = torch.diagonal(score)\n",
    "        match_score = torch.sigmoid(match_score)\n",
    "        loss = self.criterion(match_score, y)\n",
    "        return loss \n",
    "\n",
    "scoring_model = ScoringNN(4096,512)\n",
    "state_dict = torch.load(\"/kaggle/input/scoring-model/pytorch/default/1/scoring_model.pth\")\n",
    "scoring_model.load_state_dict(state_dict)\n",
    "similarity = scoring_model(\n",
    "    torch.tensor(V_answer).to(torch.float32),\n",
    "    torch.tensor(V_misconception).to(torch.float32)\n",
    ")\n",
    "score_indices = torch.argsort(similarity, dim=1, descending=True)[:, :25].numpy()\n",
    "print(score_indices[5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af1fdb9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T23:38:22.734745Z",
     "iopub.status.busy": "2024-12-02T23:38:22.734445Z",
     "iopub.status.idle": "2024-12-02T23:38:23.718736Z",
     "shell.execute_reply": "2024-12-02T23:38:23.716909Z"
    },
    "papermill": {
     "duration": 0.993987,
     "end_time": "2024-12-02T23:38:23.722622",
     "exception": false,
     "start_time": "2024-12-02T23:38:22.728635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2142  143  418 2068 1535 2078 2372 1606  891 1904 1256  320 1755  979\n",
      " 2256  167  715 1871 2398 1432  319 2139 2475 2549 1416]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForMaskedLM, AutoModel\n",
    "import sys, os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# use KNN to fit the model? -> to be seen\n",
    "# find top 25 neighbors of misconceptions as the prediction result\n",
    "def get_matches(V_topic, V_content, n_neighbors=25):\n",
    "    \n",
    "    neighbors_model = NearestNeighbors(n_neighbors=n_neighbors, metric='cosine', algorithm=\"brute\", n_jobs=-1)\n",
    "    neighbors_model.fit(V_content)\n",
    "    dists, indices = neighbors_model.kneighbors(V_topic)\n",
    "    \n",
    "    return indices\n",
    "\n",
    "use_scoring = False\n",
    "indices = get_matches(V_answer, V_misconception, n_neighbors=25)\n",
    "print(indices[5,:])\n",
    "if use_scoring:\n",
    "    indices = score_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ff0938c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T23:38:23.754800Z",
     "iopub.status.busy": "2024-12-02T23:38:23.753563Z",
     "iopub.status.idle": "2024-12-02T23:38:24.299393Z",
     "shell.execute_reply": "2024-12-02T23:38:24.298379Z"
    },
    "papermill": {
     "duration": 0.564101,
     "end_time": "2024-12-02T23:38:24.301764",
     "exception": false,
     "start_time": "2024-12-02T23:38:23.737663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "del backbone, model\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94af002f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T23:38:24.313806Z",
     "iopub.status.busy": "2024-12-02T23:38:24.313508Z",
     "iopub.status.idle": "2024-12-02T23:38:24.408991Z",
     "shell.execute_reply": "2024-12-02T23:38:24.408148Z"
    },
    "papermill": {
     "duration": 0.103226,
     "end_time": "2024-12-02T23:38:24.410774",
     "exception": false,
     "start_time": "2024-12-02T23:38:24.307548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save(\"indices.npy\", indices)\n",
    "df.to_parquet(\"df.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d7a89d",
   "metadata": {
    "papermill": {
     "duration": 0.004685,
     "end_time": "2024-12-02T23:38:24.420449",
     "exception": false,
     "start_time": "2024-12-02T23:38:24.415764",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Picking the best candidate using qwen-32b-instruct-awq\n",
    "Inspired by: https://www.kaggle.com/code/takanashihumbert/eedi-qwen-2-5-32b-awq-two-time-retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02a3425",
   "metadata": {
    "papermill": {
     "duration": 0.004665,
     "end_time": "2024-12-02T23:38:24.429860",
     "exception": false,
     "start_time": "2024-12-02T23:38:24.425195",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Using MultipleChoiceLogitsProcessor from logits-processor-zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d496d349",
   "metadata": {
    "papermill": {
     "duration": 0.004556,
     "end_time": "2024-12-02T23:38:24.439463",
     "exception": false,
     "start_time": "2024-12-02T23:38:24.434907",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"https://raw.githubusercontent.com/NVIDIA/logits-processor-zoo/refs/heads/main/docs/logo.jpg\" width=\"512\">\n",
    "</p>\n",
    "\n",
    "# logits-processor-zoo\n",
    "\n",
    "Struggling to get LLMs to follow your instructions? LogitsProcessorZoo offers a zoo of tools to use LLMs for specific tasks, beyond just grammar enforcement!\n",
    "\n",
    "## Installation\n",
    "\n",
    "```bash\n",
    "pip install logits-processor-zoo\n",
    "```\n",
    "\n",
    "## Supported Frameworks\n",
    "* transformers\n",
    "* vLLM\n",
    "* TensorRT-LLM\n",
    "\n",
    "\n",
    "For the detailed examples in each framework, please have a look at **example_notebook** directory.\n",
    "\n",
    "## Available Logits Processors\n",
    "\n",
    "### GenLengthLogitsProcessor\n",
    "A logits processor that adjusts the likelihood of the end-of-sequence (EOS) token based on the length of the generated sequence, encouraging or discouraging shorter answers.\n",
    "\n",
    "### CiteFromPromptLogitsProcessor\n",
    "A logits processor which boosts or diminishes the likelihood of tokens present in the prompt (and optionally EOS token) to encourage the model to generate tokens similar to those seen in the prompt or vice versa.\n",
    "\n",
    "### ForceLastPhraseLogitsProcessor\n",
    "A logits processor which forces LLMs to use the given phrase before they finalize their answers. Most common use cases can be providing references, thanking user with context etc.\n",
    "\n",
    "### MultipleChoiceLogitsProcessor\n",
    "A logits processor to answer multiple choice questions with one of the choices. A multiple choice question is like:\n",
    "```\n",
    "I am getting a lot of calls during the day. What is more important for me to consider when I buy a new phone?\n",
    "0. Camera\n",
    "1. Screen resolution\n",
    "2. Operating System\n",
    "3. Battery\n",
    "```\n",
    "The goal is to make LLM generate \"3\" as an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "999b23aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T23:38:24.450510Z",
     "iopub.status.busy": "2024-12-02T23:38:24.450224Z",
     "iopub.status.idle": "2024-12-02T23:38:24.457907Z",
     "shell.execute_reply": "2024-12-02T23:38:24.457123Z"
    },
    "papermill": {
     "duration": 0.015296,
     "end_time": "2024-12-02T23:38:24.459456",
     "exception": false,
     "start_time": "2024-12-02T23:38:24.444160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_vllm.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_vllm.py\n",
    "\n",
    "import vllm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import PreTrainedTokenizer, AutoTokenizer\n",
    "from typing import List\n",
    "import torch\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "import re\n",
    "\n",
    "model_path = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "def preprocess_text(x):\n",
    "    x = re.sub(\"http\\w+\", '',x)   # Delete URL\n",
    "    x = re.sub(r\"\\.+\", \".\", x)    # Replace consecutive commas and periods with one comma and period character\n",
    "    x = re.sub(r\"\\,+\", \",\", x)\n",
    "    x = re.sub(r\"\\\\\\(\", \" \", x)\n",
    "    x = re.sub(r\"\\\\\\)\", \" \", x)\n",
    "    x = re.sub(r\"[ ]{1,}\", \" \", x)\n",
    "    x = x.strip()                 # Remove empty characters at the beginning and end\n",
    "    return x\n",
    "\n",
    "# format prompt for Qwen\n",
    "PROMPT  = \"\"\"Here is a question about {SubjectName}({ConstructName}).\n",
    "Question: {Question}\n",
    "Correct Answer: {CorrectAnswer}\n",
    "Incorrect Answer: {IncorrectAnswer}\n",
    "\n",
    "You are a Mathematics teacher. Your task is to reason and identify the misconception behind the Incorrect Answer to the Question.\n",
    "Answer concisely what misconception it is to leads to getting the incorrect answer.\n",
    "Pick the ID of the correct misconception from the below:\n",
    "\n",
    "{Retrival}\n",
    "\"\"\"\n",
    "# just directly give your answers.\n",
    "\n",
    "def apply_template(row, tokenizer):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": preprocess_text(\n",
    "                PROMPT.format(\n",
    "                    ConstructName=row[\"ConstructName\"],\n",
    "                    SubjectName=row[\"SubjectName\"],\n",
    "                    Question=row[\"QuestionText\"],\n",
    "                    IncorrectAnswer=row[f\"incorrect_answer\"],\n",
    "                    CorrectAnswer=row[f\"correct_answer\"],\n",
    "                    Retrival=row[f\"retrieval\"]\n",
    "                )\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    # not sure the usage of this tokenizer\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return text\n",
    "\n",
    "\n",
    "misconception_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n",
    "\n",
    "df = pd.read_parquet(\"df.parquet\")\n",
    "indices = np.load(\"indices.npy\")\n",
    "\n",
    "model_path = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n",
    "\n",
    "# load qwen model\n",
    "llm = vllm.LLM(\n",
    "    model_path,\n",
    "    quantization=\"awq\",\n",
    "    tensor_parallel_size=2,\n",
    "    gpu_memory_utilization=0.90, \n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\", \n",
    "    enforce_eager=True,\n",
    "    max_model_len=5120,\n",
    "    disable_log_stats=True\n",
    ")\n",
    "# tokenizer\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "# format candidate\n",
    "def get_candidates(c_indices):\n",
    "    candidates = []\n",
    "\n",
    "    mis_names = misconception_df[\"MisconceptionName\"].values\n",
    "    for ix in c_indices:\n",
    "        c_names = []\n",
    "        for i, name in enumerate(mis_names[ix]):\n",
    "            c_names.append(f\"{i+1}. {name}\")\n",
    "\n",
    "        candidates.append(\"\\n\".join(c_names))\n",
    "        \n",
    "    return candidates\n",
    "\n",
    "# get last neighbor?\n",
    "survivors = indices[:, -1:]\n",
    "\n",
    "# split the 45 candidates into 4*11 + 1 sequence\n",
    "#survivor_all = []\n",
    "for i in range(3):\n",
    "    c_indices = np.concatenate([indices[:, -8*(i+1)-1:-8*i-1], survivors], axis=1)\n",
    "\n",
    "    # the ordered answers \n",
    "    df[\"retrieval\"] = get_candidates(c_indices)\n",
    "    # tokenized text （description, task, retrieval targets）\n",
    "    df[\"text\"] = df.apply(lambda row: apply_template(row, tokenizer), axis=1)\n",
    "\n",
    "    # ?x\n",
    "    print(\"Example:\")\n",
    "    print(df[\"text\"].values[0])\n",
    "    print()\n",
    "\n",
    "    # generate responses for iteratively over all samples?\n",
    "    responses = llm.generate(\n",
    "        df[\"text\"].values,\n",
    "        vllm.SamplingParams(\n",
    "            n=1,  # Number of output sequences to return for each prompt.\n",
    "            top_k=1,  # Float that controls the cumulative probability of the top tokens to consider.\n",
    "            temperature=0,  # randomness of the sampling\n",
    "            seed=777, # Seed for reprodicibility\n",
    "            skip_special_tokens=False,  # Whether to skip special tokens in the output.\n",
    "            max_tokens=1,  # Maximum number of tokens to generate per output sequence.\n",
    "            # allow the model to select the id \n",
    "            logits_processors=[MultipleChoiceLogitsProcessor(tokenizer, choices=[\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"])]\n",
    "        ),\n",
    "        use_tqdm=True\n",
    "    )\n",
    "\n",
    "    # pick the most reasonable response from the 9 candidates\n",
    "    responses = [x.outputs[0].text for x in responses]\n",
    "    df[\"response\"] = responses\n",
    "    \n",
    "    \n",
    "    llm_choices = df[\"response\"].astype(int).values - 1\n",
    "    \n",
    "    survivors = np.array([cix[best] for best, cix in zip(llm_choices, c_indices)]).reshape(-1, 1)\n",
    "    #survivor_all.append(survivors)\n",
    "#survivor_all = np.concatenate(survivor_all[::-1], axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "# seems we only pick the most important ones\n",
    "for i in range(indices.shape[0]):\n",
    "    ix = indices[i,0:25]\n",
    "    llm_choice = survivors[i,0]\n",
    "    # for l in survivor_all[i,:]:\n",
    "    #     if l not in llm_choice:\n",
    "    #         llm_choice.append(l)\n",
    "    # results.append(\" \".join([str(llm_choice)] + [str(x) for x in ix if x not in llm_choice]))\n",
    "    results.append(\" \".join([str(llm_choice)] + [str(x) for x in ix if x != llm_choice]))\n",
    "\n",
    "\n",
    "df[\"MisconceptionId\"] = results\n",
    "df.to_csv(\"submission.csv\", columns=[\"QuestionId_Answer\", \"MisconceptionId\"], index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7429cb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T23:38:24.470374Z",
     "iopub.status.busy": "2024-12-02T23:38:24.470112Z",
     "iopub.status.idle": "2024-12-02T23:40:43.472557Z",
     "shell.execute_reply": "2024-12-02T23:40:43.471612Z"
    },
    "papermill": {
     "duration": 139.009889,
     "end_time": "2024-12-02T23:40:43.474523",
     "exception": false,
     "start_time": "2024-12-02T23:38:24.464634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "WARNING 12-02 23:38:29 config.py:246] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "INFO 12-02 23:38:29 config.py:715] Defaulting to use mp for distributed inference\r\n",
      "INFO 12-02 23:38:29 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=5120, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1, use_v2_block_manager=False, enable_prefix_caching=False)\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "INFO 12-02 23:38:29 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\r\n",
      "INFO 12-02 23:38:29 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 23:38:29 selector.py:54] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=135)\u001b[0;0m INFO 12-02 23:38:29 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=135)\u001b[0;0m INFO 12-02 23:38:29 selector.py:54] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=135)\u001b[0;0m INFO 12-02 23:38:31 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=135)\u001b[0;0m INFO 12-02 23:38:31 utils.py:784] Found nccl from library libnccl.so.2\r\n",
      "INFO 12-02 23:38:31 utils.py:784] Found nccl from library libnccl.so.2\r\n",
      "INFO 12-02 23:38:31 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=135)\u001b[0;0m INFO 12-02 23:38:31 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 12-02 23:38:31 custom_all_reduce_utils.py:202] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 12-02 23:38:38 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=135)\u001b[0;0m INFO 12-02 23:38:38 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 12-02 23:38:38 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7a3c2cd9e590>, local_subscribe_port=52895, local_sync_port=40103, remote_subscribe_port=None, remote_sync_port=None)\r\n",
      "INFO 12-02 23:38:38 model_runner.py:680] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=135)\u001b[0;0m INFO 12-02 23:38:38 model_runner.py:680] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\r\n",
      "INFO 12-02 23:38:38 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=135)\u001b[0;0m INFO 12-02 23:38:38 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=135)\u001b[0;0m INFO 12-02 23:38:38 selector.py:54] Using XFormers backend.\r\n",
      "INFO 12-02 23:38:38 selector.py:54] Using XFormers backend.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:15<01:02, 15.68s/it]\r\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:32<00:49, 16.56s/it]\r\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:51<00:34, 17.28s/it]\r\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [01:09<00:17, 17.75s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [01:29<00:00, 18.41s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [01:29<00:00, 17.81s/it]\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=135)\u001b[0;0m INFO 12-02 23:40:08 model_runner.py:692] Loading model weights took 9.0934 GB\r\n",
      "INFO 12-02 23:40:08 model_runner.py:692] Loading model weights took 9.0934 GB\r\n",
      "INFO 12-02 23:40:17 distributed_gpu_executor.py:56] # GPU blocks: 795, # CPU blocks: 2048\r\n",
      "Example:\r\n",
      "<|im_start|>system\r\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "Here is a question about BIDMAS(Use the order of operations to carry out calculations involving powers).\r\n",
      "Question: \\[\r\n",
      "3 \\times 2+4-5\r\n",
      "\\]\r\n",
      "Where do the brackets need to go to make the answer equal 13 ?\r\n",
      "Correct Answer: 3 \\times(2+4)-5 \r\n",
      "Incorrect Answer: 3 \\times 2+(4-5) \r\n",
      "\r\n",
      "You are a Mathematics teacher. Your task is to reason and identify the misconception behind the Incorrect Answer to the Question.\r\n",
      "Answer concisely what misconception it is to leads to getting the incorrect answer.\r\n",
      "Pick the ID of the correct misconception from the below:\r\n",
      "\r\n",
      "1. Does not include brackets when required\r\n",
      "2. Confuses the order of operations, believes addition comes before division\r\n",
      "3. Performs addition ahead of subtraction\r\n",
      "4. Answers order of operations questions with brackets as if the brackets are not there\r\n",
      "5. Has not realised that the answer may be changed by the insertion of brackets\r\n",
      "6. Has added the powers rather than multiplying them\r\n",
      "7. Does not reverse the order of operations when solving an equation\r\n",
      "8. Thinks the order of events does not matter in a question about combined probability with no replacement\r\n",
      "9. Ordered from largest to smallest<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "\r\n",
      "\r\n",
      "Processed prompts: 100%|█| 9/9 [00:06<00:00,  1.47it/s, est. speed input: 467.35\r\n",
      "Example:\r\n",
      "<|im_start|>system\r\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "Here is a question about BIDMAS(Use the order of operations to carry out calculations involving powers).\r\n",
      "Question: \\[\r\n",
      "3 \\times 2+4-5\r\n",
      "\\]\r\n",
      "Where do the brackets need to go to make the answer equal 13 ?\r\n",
      "Correct Answer: 3 \\times(2+4)-5 \r\n",
      "Incorrect Answer: 3 \\times 2+(4-5) \r\n",
      "\r\n",
      "You are a Mathematics teacher. Your task is to reason and identify the misconception behind the Incorrect Answer to the Question.\r\n",
      "Answer concisely what misconception it is to leads to getting the incorrect answer.\r\n",
      "Pick the ID of the correct misconception from the below:\r\n",
      "\r\n",
      "1. Performs addition ahead of multiplication\r\n",
      "2. Misunderstands order of operations in algebraic expressions\r\n",
      "3. Confuses the order of operations, believes subtraction comes before multiplication \r\n",
      "4. Carries out operations from left to right regardless of priority order, unless brackets are used\r\n",
      "5. Performs subtraction right to left if priority order means doing a calculation to the right first\r\n",
      "6. May have made a calculation error using the order of operations\r\n",
      "7. Believes addition comes before indices, in orders of operation\r\n",
      "8. Applies BIDMAS in strict order (does not realize addition and subtraction, and multiplication and division, are of equal priority)\r\n",
      "9. Has not realised that the answer may be changed by the insertion of brackets<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "\r\n",
      "\r\n",
      "Processed prompts: 100%|█| 9/9 [00:05<00:00,  1.61it/s, est. speed input: 513.35\r\n",
      "Example:\r\n",
      "<|im_start|>system\r\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "Here is a question about BIDMAS(Use the order of operations to carry out calculations involving powers).\r\n",
      "Question: \\[\r\n",
      "3 \\times 2+4-5\r\n",
      "\\]\r\n",
      "Where do the brackets need to go to make the answer equal 13 ?\r\n",
      "Correct Answer: 3 \\times(2+4)-5 \r\n",
      "Incorrect Answer: 3 \\times 2+(4-5) \r\n",
      "\r\n",
      "You are a Mathematics teacher. Your task is to reason and identify the misconception behind the Incorrect Answer to the Question.\r\n",
      "Answer concisely what misconception it is to leads to getting the incorrect answer.\r\n",
      "Pick the ID of the correct misconception from the below:\r\n",
      "\r\n",
      "1. Carries out operations from right to left regardless of priority order\r\n",
      "2. Carries out operations from left to right regardless of priority order\r\n",
      "3. Believes order of operations does not affect the answer to a calculation\r\n",
      "4. Confuses the order of operations, believes addition comes before multiplication \r\n",
      "5. Performs subtraction in wrong order\r\n",
      "6. Does not follow the arrows through a function machine, changes the order of the operations asked.\r\n",
      "7. Inserts brackets but not changed order of operation\r\n",
      "8. Performs addition ahead of any other operation\r\n",
      "9. Misunderstands order of operations in algebraic expressions<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "\r\n",
      "\r\n",
      "Processed prompts: 100%|█| 9/9 [00:06<00:00,  1.46it/s, est. speed input: 483.86\r\n",
      "[rank0]:[W CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\r\n"
     ]
    }
   ],
   "source": [
    "!python run_vllm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e65010ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T23:40:43.488615Z",
     "iopub.status.busy": "2024-12-02T23:40:43.488297Z",
     "iopub.status.idle": "2024-12-02T23:40:43.502571Z",
     "shell.execute_reply": "2024-12-02T23:40:43.501723Z"
    },
    "papermill": {
     "duration": 0.023211,
     "end_time": "2024-12-02T23:40:43.504180",
     "exception": false,
     "start_time": "2024-12-02T23:40:43.480969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>MisconceptionId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869_B</td>\n",
       "      <td>1345 706 1507 2532 1672 1054 77 1516 328 2586 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1869_C</td>\n",
       "      <td>1345 2306 1054 1941 2532 1963 2488 706 1672 25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1869_D</td>\n",
       "      <td>2532 1672 706 1507 158 77 328 15 1516 1345 258...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870_A</td>\n",
       "      <td>1755 2142 143 418 2068 2078 1535 2372 891 320 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1870_B</td>\n",
       "      <td>1755 143 891 363 2398 2078 715 979 413 2549 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1870_C</td>\n",
       "      <td>1755 2142 143 418 2068 1535 2078 2372 1606 891...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1871_A</td>\n",
       "      <td>1287 1073 1408 2408 2439 1059 1923 1975 906 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1871_C</td>\n",
       "      <td>1287 1073 1408 2408 2439 1059 1923 906 2188 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1871_D</td>\n",
       "      <td>1287 1073 2439 1059 1408 1923 2408 2471 906 39...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QuestionId_Answer                                    MisconceptionId\n",
       "0            1869_B  1345 706 1507 2532 1672 1054 77 1516 328 2586 ...\n",
       "1            1869_C  1345 2306 1054 1941 2532 1963 2488 706 1672 25...\n",
       "2            1869_D  2532 1672 706 1507 158 77 328 15 1516 1345 258...\n",
       "3            1870_A  1755 2142 143 418 2068 2078 1535 2372 891 320 ...\n",
       "4            1870_B  1755 143 891 363 2398 2078 715 979 413 2549 18...\n",
       "5            1870_C  1755 2142 143 418 2068 1535 2078 2372 1606 891...\n",
       "6            1871_A  1287 1073 1408 2408 2439 1059 1923 1975 906 13...\n",
       "7            1871_C  1287 1073 1408 2408 2439 1059 1923 906 2188 21...\n",
       "8            1871_D  1287 1073 2439 1059 1408 1923 2408 2471 906 39..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5efa0ab",
   "metadata": {
    "papermill": {
     "duration": 0.006052,
     "end_time": "2024-12-02T23:40:43.517134",
     "exception": false,
     "start_time": "2024-12-02T23:40:43.511082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3566a4af",
   "metadata": {
    "papermill": {
     "duration": 0.006019,
     "end_time": "2024-12-02T23:40:43.529307",
     "exception": false,
     "start_time": "2024-12-02T23:40:43.523288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9738540,
     "sourceId": 82695,
     "sourceType": "competition"
    },
    {
     "datasetId": 4871830,
     "sourceId": 8218776,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5297895,
     "sourceId": 8897601,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5251603,
     "sourceId": 9094368,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5920031,
     "sourceId": 9688062,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5957531,
     "sourceId": 9734430,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6117312,
     "sourceId": 9948011,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4581967,
     "sourceId": 10079193,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 200567623,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 123481,
     "modelInstanceId": 99392,
     "sourceId": 118192,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 179411,
     "modelInstanceId": 156978,
     "sourceId": 184170,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 940.923832,
   "end_time": "2024-12-02T23:40:47.029740",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-02T23:25:06.105908",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01c9d2e87bd04d6996d41093227362af": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f04293554373451283a955836b8b30df",
       "placeholder": "​",
       "style": "IPY_MODEL_881b566d80cc40a39ccaaf3ce10c2912",
       "value": " 3/3 [01:05&lt;00:00, 21.35s/it]"
      }
     },
     "04eac4dd52ed43138c78d290dfc5b021": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "0951e87448a649c6afd852bf3b069072": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3a44e1e68e7a410192899ec24cf0b764",
       "placeholder": "​",
       "style": "IPY_MODEL_240ff53ce4b441a9bd824a984abf2fa4",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "0e9ef606096c413097a7d23fa90fcecc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "10b99e0ef6574eb2bfbf7f17274f8776": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "125db31b56f840948209aebbe4fe9986": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2031efd18edb43c98a1e79c15714e470": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0951e87448a649c6afd852bf3b069072",
        "IPY_MODEL_f3bec730ffe04b01923d6c51a848d22b",
        "IPY_MODEL_01c9d2e87bd04d6996d41093227362af"
       ],
       "layout": "IPY_MODEL_6ab590a7e0714ab8bd814ee8a87c529b"
      }
     },
     "240ff53ce4b441a9bd824a984abf2fa4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "25ac997e1544469c8673cbdf7e9696ed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2f484cc64acd4403905ea0ad4d0a2a29": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "3a44e1e68e7a410192899ec24cf0b764": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4208c0e310c24670b1a2faff451c36cb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "53eb27d2f0974e3eb9db6ec54919af09": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5790d4578aea41728d12d7e8597c9aa4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5d5e9e8e3bb4440c878161c9ef158d2d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6ab590a7e0714ab8bd814ee8a87c529b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "881b566d80cc40a39ccaaf3ce10c2912": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8f1fffeea8d44fe99c0980a57412dbc2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e00a047c54794ecca23ddf51bbb3fae0",
       "placeholder": "​",
       "style": "IPY_MODEL_0e9ef606096c413097a7d23fa90fcecc",
       "value": " 1/1 [00:18&lt;00:00, 18.72s/it]"
      }
     },
     "8f4678ab2ee048c78fb856a95e032ec3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "94059fa6bfae41c4beaf6232bf4653aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_53eb27d2f0974e3eb9db6ec54919af09",
       "placeholder": "​",
       "style": "IPY_MODEL_25ac997e1544469c8673cbdf7e9696ed",
       "value": " 162/162 [07:17&lt;00:00,  1.86s/it]"
      }
     },
     "988f59a369b84c8cb81a30a126172afb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_125db31b56f840948209aebbe4fe9986",
       "placeholder": "​",
       "style": "IPY_MODEL_c3a3d2e85c8c46e2aed0559d1c8ad48e",
       "value": "Batches: 100%"
      }
     },
     "9998889e0928439a8a79167614b132ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_988f59a369b84c8cb81a30a126172afb",
        "IPY_MODEL_cfcd654572d94811b0b09c3a43c9b288",
        "IPY_MODEL_94059fa6bfae41c4beaf6232bf4653aa"
       ],
       "layout": "IPY_MODEL_8f4678ab2ee048c78fb856a95e032ec3"
      }
     },
     "9cbd07886d14463f8f293d9091dfe947": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aaa7ea82dafc4fd7acb857431dae9173": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_dce42384449c40bc96b1d2038327d475",
        "IPY_MODEL_d08e22c9deee4601b0aef7bacc51dafd",
        "IPY_MODEL_8f1fffeea8d44fe99c0980a57412dbc2"
       ],
       "layout": "IPY_MODEL_9cbd07886d14463f8f293d9091dfe947"
      }
     },
     "b6460096564f4f1ca71a3e0310f05bd6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c3a3d2e85c8c46e2aed0559d1c8ad48e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "cfcd654572d94811b0b09c3a43c9b288": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5d5e9e8e3bb4440c878161c9ef158d2d",
       "max": 162.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5790d4578aea41728d12d7e8597c9aa4",
       "value": 162.0
      }
     },
     "d08e22c9deee4601b0aef7bacc51dafd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_10b99e0ef6574eb2bfbf7f17274f8776",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b6460096564f4f1ca71a3e0310f05bd6",
       "value": 1.0
      }
     },
     "dce42384449c40bc96b1d2038327d475": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4208c0e310c24670b1a2faff451c36cb",
       "placeholder": "​",
       "style": "IPY_MODEL_2f484cc64acd4403905ea0ad4d0a2a29",
       "value": "Batches: 100%"
      }
     },
     "e00a047c54794ecca23ddf51bbb3fae0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f04293554373451283a955836b8b30df": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f3bec730ffe04b01923d6c51a848d22b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f651fbf9f67648d7b86c9c567e8c2cca",
       "max": 3.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_04eac4dd52ed43138c78d290dfc5b021",
       "value": 3.0
      }
     },
     "f651fbf9f67648d7b86c9c567e8c2cca": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
